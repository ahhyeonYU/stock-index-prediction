{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "from feature_selection import get_data\n",
    "import FinanceDataReader as fdr\n",
    "import datetime\n",
    "\n",
    "#%matplotlib inline\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#plt.rcParams['font.family'] = 'NanumGothic'\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Bidirectional\n",
    "from tensorflow.keras.losses import Huber\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "weight_name = os.path.join('tmp_checkpoint.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, scaler_kospi = get_data('D:/stock-market-index-prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>한국은행_기준금리</th>\n",
       "      <th>KOSPI</th>\n",
       "      <th>KOSDAQ</th>\n",
       "      <th>CD_91일</th>\n",
       "      <th>국고채_10년</th>\n",
       "      <th>회사채_3년_AA</th>\n",
       "      <th>KOSPI_거래대금_일평균</th>\n",
       "      <th>KOSDAQ_거래대금_일평균</th>\n",
       "      <th>KOSPI_주가이익비율_3</th>\n",
       "      <th>기관투자자_순매수</th>\n",
       "      <th>개인_순매수</th>\n",
       "      <th>투자자_예탁금</th>\n",
       "      <th>파생상품거래_예수금_1</th>\n",
       "      <th>RP</th>\n",
       "      <th>위탁매매_미수금</th>\n",
       "      <th>신용융자_잔고_2</th>\n",
       "      <th>거래대금_일평균_CALL_옵션</th>\n",
       "      <th>거래대금_일평균_PUT_옵션</th>\n",
       "      <th>KOSPI_BINARY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.150785</td>\n",
       "      <td>0.523719</td>\n",
       "      <td>0.651852</td>\n",
       "      <td>0.905858</td>\n",
       "      <td>0.557803</td>\n",
       "      <td>0.133986</td>\n",
       "      <td>0.128401</td>\n",
       "      <td>0.110236</td>\n",
       "      <td>0.660705</td>\n",
       "      <td>0.172458</td>\n",
       "      <td>0.080661</td>\n",
       "      <td>0.019894</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>0.994870</td>\n",
       "      <td>0.005722</td>\n",
       "      <td>0.212613</td>\n",
       "      <td>0.125723</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.138142</td>\n",
       "      <td>0.512937</td>\n",
       "      <td>0.670370</td>\n",
       "      <td>0.843096</td>\n",
       "      <td>0.533237</td>\n",
       "      <td>0.073347</td>\n",
       "      <td>0.090498</td>\n",
       "      <td>0.108993</td>\n",
       "      <td>0.721468</td>\n",
       "      <td>0.211550</td>\n",
       "      <td>0.064424</td>\n",
       "      <td>0.008211</td>\n",
       "      <td>0.001543</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005518</td>\n",
       "      <td>0.204223</td>\n",
       "      <td>0.150988</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.132774</td>\n",
       "      <td>0.489471</td>\n",
       "      <td>0.674074</td>\n",
       "      <td>0.872385</td>\n",
       "      <td>0.528902</td>\n",
       "      <td>0.034976</td>\n",
       "      <td>0.068318</td>\n",
       "      <td>0.104849</td>\n",
       "      <td>0.725261</td>\n",
       "      <td>0.208304</td>\n",
       "      <td>0.050511</td>\n",
       "      <td>0.001297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.731455</td>\n",
       "      <td>0.004496</td>\n",
       "      <td>0.101806</td>\n",
       "      <td>0.094471</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.159694</td>\n",
       "      <td>0.517151</td>\n",
       "      <td>0.685185</td>\n",
       "      <td>0.878661</td>\n",
       "      <td>0.520231</td>\n",
       "      <td>0.064868</td>\n",
       "      <td>0.077548</td>\n",
       "      <td>0.133858</td>\n",
       "      <td>0.711395</td>\n",
       "      <td>0.185593</td>\n",
       "      <td>0.083090</td>\n",
       "      <td>0.002425</td>\n",
       "      <td>0.003498</td>\n",
       "      <td>0.805738</td>\n",
       "      <td>0.005678</td>\n",
       "      <td>0.187208</td>\n",
       "      <td>0.086090</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.114015</td>\n",
       "      <td>0.441979</td>\n",
       "      <td>0.690741</td>\n",
       "      <td>0.826360</td>\n",
       "      <td>0.492775</td>\n",
       "      <td>0.052435</td>\n",
       "      <td>0.050821</td>\n",
       "      <td>0.098632</td>\n",
       "      <td>0.784206</td>\n",
       "      <td>0.261097</td>\n",
       "      <td>0.043003</td>\n",
       "      <td>0.047697</td>\n",
       "      <td>0.003992</td>\n",
       "      <td>0.462610</td>\n",
       "      <td>0.004466</td>\n",
       "      <td>0.194498</td>\n",
       "      <td>0.152607</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.854051</td>\n",
       "      <td>0.937060</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.240586</td>\n",
       "      <td>0.095376</td>\n",
       "      <td>0.384382</td>\n",
       "      <td>0.681778</td>\n",
       "      <td>0.239536</td>\n",
       "      <td>0.717829</td>\n",
       "      <td>0.320441</td>\n",
       "      <td>0.941931</td>\n",
       "      <td>0.747508</td>\n",
       "      <td>0.968454</td>\n",
       "      <td>0.094060</td>\n",
       "      <td>0.963737</td>\n",
       "      <td>0.113237</td>\n",
       "      <td>0.148818</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.795102</td>\n",
       "      <td>0.900527</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.232218</td>\n",
       "      <td>0.122832</td>\n",
       "      <td>0.382876</td>\n",
       "      <td>0.795261</td>\n",
       "      <td>0.219644</td>\n",
       "      <td>0.637877</td>\n",
       "      <td>0.159406</td>\n",
       "      <td>0.929911</td>\n",
       "      <td>0.776359</td>\n",
       "      <td>0.948874</td>\n",
       "      <td>0.152658</td>\n",
       "      <td>0.940149</td>\n",
       "      <td>0.100905</td>\n",
       "      <td>0.119428</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.857171</td>\n",
       "      <td>0.994048</td>\n",
       "      <td>0.118519</td>\n",
       "      <td>0.196653</td>\n",
       "      <td>0.111272</td>\n",
       "      <td>0.307691</td>\n",
       "      <td>0.701862</td>\n",
       "      <td>0.176129</td>\n",
       "      <td>0.800442</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.954787</td>\n",
       "      <td>0.704850</td>\n",
       "      <td>0.864489</td>\n",
       "      <td>0.151241</td>\n",
       "      <td>0.925292</td>\n",
       "      <td>0.169232</td>\n",
       "      <td>0.098858</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.716455</td>\n",
       "      <td>0.773606</td>\n",
       "      <td>0.140741</td>\n",
       "      <td>0.259414</td>\n",
       "      <td>0.143064</td>\n",
       "      <td>0.364686</td>\n",
       "      <td>0.576084</td>\n",
       "      <td>0.152093</td>\n",
       "      <td>0.581643</td>\n",
       "      <td>0.374699</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.822928</td>\n",
       "      <td>0.935625</td>\n",
       "      <td>0.139390</td>\n",
       "      <td>0.867558</td>\n",
       "      <td>0.156128</td>\n",
       "      <td>0.183374</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.732501</td>\n",
       "      <td>0.784826</td>\n",
       "      <td>0.161111</td>\n",
       "      <td>0.301255</td>\n",
       "      <td>0.177746</td>\n",
       "      <td>0.350782</td>\n",
       "      <td>0.463611</td>\n",
       "      <td>0.159552</td>\n",
       "      <td>0.624192</td>\n",
       "      <td>0.238240</td>\n",
       "      <td>0.888828</td>\n",
       "      <td>0.768404</td>\n",
       "      <td>0.940553</td>\n",
       "      <td>0.104835</td>\n",
       "      <td>0.835914</td>\n",
       "      <td>0.179183</td>\n",
       "      <td>0.166157</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>194 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     한국은행_기준금리     KOSPI    KOSDAQ    CD_91일   국고채_10년  회사채_3년_AA  \\\n",
       "0     0.684211  0.150785  0.523719  0.651852  0.905858   0.557803   \n",
       "1     0.736842  0.138142  0.512937  0.670370  0.843096   0.533237   \n",
       "2     0.736842  0.132774  0.489471  0.674074  0.872385   0.528902   \n",
       "3     0.736842  0.159694  0.517151  0.685185  0.878661   0.520231   \n",
       "4     0.736842  0.114015  0.441979  0.690741  0.826360   0.492775   \n",
       "..         ...       ...       ...       ...       ...        ...   \n",
       "189   0.052632  0.854051  0.937060  0.083333  0.240586   0.095376   \n",
       "190   0.105263  0.795102  0.900527  0.100000  0.232218   0.122832   \n",
       "191   0.105263  0.857171  0.994048  0.118519  0.196653   0.111272   \n",
       "192   0.157895  0.716455  0.773606  0.140741  0.259414   0.143064   \n",
       "193   0.157895  0.732501  0.784826  0.161111  0.301255   0.177746   \n",
       "\n",
       "     KOSPI_거래대금_일평균  KOSDAQ_거래대금_일평균  KOSPI_주가이익비율_3  기관투자자_순매수    개인_순매수  \\\n",
       "0          0.133986         0.128401        0.110236   0.660705  0.172458   \n",
       "1          0.073347         0.090498        0.108993   0.721468  0.211550   \n",
       "2          0.034976         0.068318        0.104849   0.725261  0.208304   \n",
       "3          0.064868         0.077548        0.133858   0.711395  0.185593   \n",
       "4          0.052435         0.050821        0.098632   0.784206  0.261097   \n",
       "..              ...              ...             ...        ...       ...   \n",
       "189        0.384382         0.681778        0.239536   0.717829  0.320441   \n",
       "190        0.382876         0.795261        0.219644   0.637877  0.159406   \n",
       "191        0.307691         0.701862        0.176129   0.800442  0.000000   \n",
       "192        0.364686         0.576084        0.152093   0.581643  0.374699   \n",
       "193        0.350782         0.463611        0.159552   0.624192  0.238240   \n",
       "\n",
       "      투자자_예탁금  파생상품거래_예수금_1        RP  위탁매매_미수금  신용융자_잔고_2  거래대금_일평균_CALL_옵션  \\\n",
       "0    0.080661      0.019894  0.000487  0.994870   0.005722          0.212613   \n",
       "1    0.064424      0.008211  0.001543  1.000000   0.005518          0.204223   \n",
       "2    0.050511      0.001297  0.000000  0.731455   0.004496          0.101806   \n",
       "3    0.083090      0.002425  0.003498  0.805738   0.005678          0.187208   \n",
       "4    0.043003      0.047697  0.003992  0.462610   0.004466          0.194498   \n",
       "..        ...           ...       ...       ...        ...               ...   \n",
       "189  0.941931      0.747508  0.968454  0.094060   0.963737          0.113237   \n",
       "190  0.929911      0.776359  0.948874  0.152658   0.940149          0.100905   \n",
       "191  0.954787      0.704850  0.864489  0.151241   0.925292          0.169232   \n",
       "192  1.000000      0.822928  0.935625  0.139390   0.867558          0.156128   \n",
       "193  0.888828      0.768404  0.940553  0.104835   0.835914          0.179183   \n",
       "\n",
       "     거래대금_일평균_PUT_옵션  KOSPI_BINARY  \n",
       "0           0.125723           0.0  \n",
       "1           0.150988           0.0  \n",
       "2           0.094471           1.0  \n",
       "3           0.086090           0.0  \n",
       "4           0.152607           0.0  \n",
       "..               ...           ...  \n",
       "189         0.148818           0.0  \n",
       "190         0.119428           1.0  \n",
       "191         0.098858           0.0  \n",
       "192         0.183374           1.0  \n",
       "193         0.166157           1.0  \n",
       "\n",
       "[194 rows x 19 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 10\n",
    "\n",
    "# bilstm 모델이 필요로하는 input 형식으로 데이터셋 구성\n",
    "def make_dataset(data, label, window_size):\n",
    "    feature_list = []\n",
    "    label_list = []\n",
    "    for i in range(len(data) - window_size):\n",
    "        feature_list.append(np.array(data.iloc[i:i+window_size]))\n",
    "        label_list.append(np.array(label.iloc[i+window_size-1])) # -1을 한 이유: 이미 한달 뒤 변동 여부가 KOSPI_BINARY 칼럼에 값으로 들어가 있음\n",
    "    return np.array(feature_list), np.array(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 종속변수 정의\n",
    "target_col = 'KOSPI_BINARY'\n",
    "\n",
    "# 독립변수, 종속변수별 데이터셋 분리\n",
    "feature_cols = list(df.columns)\n",
    "feature_cols.remove(target_col)\n",
    "label_cols = [target_col]\n",
    "\n",
    "train_feature = df[feature_cols]\n",
    "train_label = df[label_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, validation 데이터셋 구축\n",
    "train_feature, train_label = make_dataset(train_feature, train_label, window_size)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(train_feature, train_label, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((147, 10, 18), (37, 10, 18))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\loven\\Anaconda3\\envs\\allocation\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\loven\\Anaconda3\\envs\\allocation\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\loven\\Anaconda3\\envs\\allocation\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\loven\\Anaconda3\\envs\\allocation\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "# 모델링\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(16, \n",
    "               input_shape=(train_feature.shape[1], train_feature.shape[2]), \n",
    "               activation='sigmoid',\n",
    "               return_sequences=False))\n",
    "          )\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 147 samples, validate on 37 samples\n",
      "WARNING:tensorflow:From C:\\Users\\loven\\Anaconda3\\envs\\allocation\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/200\n",
      "128/147 [=========================>....] - ETA: 0s - loss: 0.2285\n",
      "Epoch 00001: val_loss improved from inf to 0.20187, saving model to tmp_checkpoint.h5\n",
      "147/147 [==============================] - 3s 19ms/sample - loss: 0.2261 - val_loss: 0.2019\n",
      "Epoch 2/200\n",
      "128/147 [=========================>....] - ETA: 0s - loss: 0.1886\n",
      "Epoch 00002: val_loss improved from 0.20187 to 0.16668, saving model to tmp_checkpoint.h5\n",
      "147/147 [==============================] - 0s 1ms/sample - loss: 0.1848 - val_loss: 0.1667\n",
      "Epoch 3/200\n",
      "128/147 [=========================>....] - ETA: 0s - loss: 0.1614\n",
      "Epoch 00003: val_loss improved from 0.16668 to 0.14600, saving model to tmp_checkpoint.h5\n",
      "147/147 [==============================] - 0s 769us/sample - loss: 0.1586 - val_loss: 0.1460\n",
      "Epoch 4/200\n",
      "112/147 [=====================>........] - ETA: 0s - loss: 0.1416\n",
      "Epoch 00004: val_loss improved from 0.14600 to 0.13502, saving model to tmp_checkpoint.h5\n",
      "147/147 [==============================] - 0s 885us/sample - loss: 0.1426 - val_loss: 0.1350\n",
      "Epoch 5/200\n",
      " 96/147 [==================>...........] - ETA: 0s - loss: 0.1370\n",
      "Epoch 00005: val_loss improved from 0.13502 to 0.13074, saving model to tmp_checkpoint.h5\n",
      "147/147 [==============================] - 0s 896us/sample - loss: 0.1340 - val_loss: 0.1307\n",
      "Epoch 6/200\n",
      " 80/147 [===============>..............] - ETA: 0s - loss: 0.1331\n",
      "Epoch 00006: val_loss improved from 0.13074 to 0.13008, saving model to tmp_checkpoint.h5\n",
      "147/147 [==============================] - 0s 1ms/sample - loss: 0.1330 - val_loss: 0.1301\n",
      "Epoch 7/200\n",
      "128/147 [=========================>....] - ETA: 0s - loss: 0.1322\n",
      "Epoch 00007: val_loss improved from 0.13008 to 0.12936, saving model to tmp_checkpoint.h5\n",
      "147/147 [==============================] - 0s 1ms/sample - loss: 0.1322 - val_loss: 0.1294\n",
      "Epoch 8/200\n",
      " 96/147 [==================>...........] - ETA: 0s - loss: 0.1328\n",
      "Epoch 00008: val_loss improved from 0.12936 to 0.12844, saving model to tmp_checkpoint.h5\n",
      "147/147 [==============================] - 0s 964us/sample - loss: 0.1312 - val_loss: 0.1284\n",
      "Epoch 9/200\n",
      " 96/147 [==================>...........] - ETA: 0s - loss: 0.1352\n",
      "Epoch 00009: val_loss improved from 0.12844 to 0.12775, saving model to tmp_checkpoint.h5\n",
      "147/147 [==============================] - 0s 923us/sample - loss: 0.1300 - val_loss: 0.1278\n",
      "Epoch 10/200\n",
      " 96/147 [==================>...........] - ETA: 0s - loss: 0.1260\n",
      "Epoch 00010: val_loss improved from 0.12775 to 0.12756, saving model to tmp_checkpoint.h5\n",
      "147/147 [==============================] - 0s 914us/sample - loss: 0.1291 - val_loss: 0.1276\n",
      "Epoch 11/200\n",
      "112/147 [=====================>........] - ETA: 0s - loss: 0.1269\n",
      "Epoch 00011: val_loss improved from 0.12756 to 0.12737, saving model to tmp_checkpoint.h5\n",
      "147/147 [==============================] - 0s 838us/sample - loss: 0.1286 - val_loss: 0.1274\n",
      "Epoch 12/200\n",
      " 80/147 [===============>..............] - ETA: 0s - loss: 0.1298\n",
      "Epoch 00012: val_loss did not improve from 0.12737\n",
      "147/147 [==============================] - 0s 857us/sample - loss: 0.1280 - val_loss: 0.1275\n",
      "Epoch 13/200\n",
      "144/147 [============================>.] - ETA: 0s - loss: 0.1279\n",
      "Epoch 00013: val_loss improved from 0.12737 to 0.12714, saving model to tmp_checkpoint.h5\n",
      "147/147 [==============================] - 0s 821us/sample - loss: 0.1277 - val_loss: 0.1271\n",
      "Epoch 14/200\n",
      "144/147 [============================>.] - ETA: 0s - loss: 0.1269\n",
      "Epoch 00014: val_loss improved from 0.12714 to 0.12671, saving model to tmp_checkpoint.h5\n",
      "147/147 [==============================] - 0s 1ms/sample - loss: 0.1272 - val_loss: 0.1267\n",
      "Epoch 15/200\n",
      " 96/147 [==================>...........] - ETA: 0s - loss: 0.1279\n",
      "Epoch 00015: val_loss improved from 0.12671 to 0.12642, saving model to tmp_checkpoint.h5\n",
      "147/147 [==============================] - 0s 1ms/sample - loss: 0.1267 - val_loss: 0.1264\n",
      "Epoch 16/200\n",
      "144/147 [============================>.] - ETA: 0s - loss: 0.1257\n",
      "Epoch 00016: val_loss improved from 0.12642 to 0.12610, saving model to tmp_checkpoint.h5\n",
      "147/147 [==============================] - 0s 1ms/sample - loss: 0.1267 - val_loss: 0.1261\n",
      "Epoch 17/200\n",
      " 96/147 [==================>...........] - ETA: 0s - loss: 0.1238\n",
      "Epoch 00017: val_loss improved from 0.12610 to 0.12597, saving model to tmp_checkpoint.h5\n",
      "147/147 [==============================] - 0s 954us/sample - loss: 0.1255 - val_loss: 0.1260\n",
      "Epoch 18/200\n",
      " 96/147 [==================>...........] - ETA: 0s - loss: 0.1236\n",
      "Epoch 00018: val_loss did not improve from 0.12597\n",
      "147/147 [==============================] - 0s 778us/sample - loss: 0.1255 - val_loss: 0.1260\n",
      "Epoch 19/200\n",
      "128/147 [=========================>....] - ETA: 0s - loss: 0.1260\n",
      "Epoch 00019: val_loss improved from 0.12597 to 0.12547, saving model to tmp_checkpoint.h5\n",
      "147/147 [==============================] - 0s 833us/sample - loss: 0.1253 - val_loss: 0.1255\n",
      "Epoch 20/200\n",
      "112/147 [=====================>........] - ETA: 0s - loss: 0.1234\n",
      "Epoch 00020: val_loss improved from 0.12547 to 0.12516, saving model to tmp_checkpoint.h5\n",
      "147/147 [==============================] - 0s 920us/sample - loss: 0.1247 - val_loss: 0.1252\n",
      "Epoch 21/200\n",
      "144/147 [============================>.] - ETA: 0s - loss: 0.1253\n",
      "Epoch 00021: val_loss did not improve from 0.12516\n",
      "147/147 [==============================] - 0s 940us/sample - loss: 0.1247 - val_loss: 0.1253\n",
      "Epoch 22/200\n",
      "128/147 [=========================>....] - ETA: 0s - loss: 0.1241\n",
      "Epoch 00022: val_loss did not improve from 0.12516\n",
      "147/147 [==============================] - 0s 579us/sample - loss: 0.1239 - val_loss: 0.1253\n",
      "Epoch 23/200\n",
      "128/147 [=========================>....] - ETA: 0s - loss: 0.1228\n",
      "Epoch 00023: val_loss did not improve from 0.12516\n",
      "147/147 [==============================] - 0s 605us/sample - loss: 0.1235 - val_loss: 0.1252\n",
      "Epoch 24/200\n",
      "128/147 [=========================>....] - ETA: 0s - loss: 0.1232\n",
      "Epoch 00024: val_loss improved from 0.12516 to 0.12507, saving model to tmp_checkpoint.h5\n",
      "147/147 [==============================] - 0s 811us/sample - loss: 0.1236 - val_loss: 0.1251\n",
      "Epoch 25/200\n",
      "128/147 [=========================>....] - ETA: 0s - loss: 0.1217\n",
      "Epoch 00025: val_loss improved from 0.12507 to 0.12497, saving model to tmp_checkpoint.h5\n",
      "147/147 [==============================] - 0s 822us/sample - loss: 0.1234 - val_loss: 0.1250\n",
      "Epoch 26/200\n",
      "112/147 [=====================>........] - ETA: 0s - loss: 0.1230\n",
      "Epoch 00026: val_loss improved from 0.12497 to 0.12438, saving model to tmp_checkpoint.h5\n",
      "147/147 [==============================] - 0s 890us/sample - loss: 0.1225 - val_loss: 0.1244\n",
      "Epoch 27/200\n",
      " 80/147 [===============>..............] - ETA: 0s - loss: 0.1256\n",
      "Epoch 00027: val_loss improved from 0.12438 to 0.12408, saving model to tmp_checkpoint.h5\n",
      "147/147 [==============================] - 0s 1ms/sample - loss: 0.1220 - val_loss: 0.1241\n",
      "Epoch 28/200\n",
      " 64/147 [============>.................] - ETA: 0s - loss: 0.1268\n",
      "Epoch 00028: val_loss improved from 0.12408 to 0.12400, saving model to tmp_checkpoint.h5\n",
      "147/147 [==============================] - 0s 1ms/sample - loss: 0.1219 - val_loss: 0.1240\n",
      "Epoch 29/200\n",
      "144/147 [============================>.] - ETA: 0s - loss: 0.1221\n",
      "Epoch 00029: val_loss did not improve from 0.12400\n",
      "147/147 [==============================] - 0s 1ms/sample - loss: 0.1217 - val_loss: 0.1246\n",
      "Epoch 30/200\n",
      "128/147 [=========================>....] - ETA: 0s - loss: 0.1215\n",
      "Epoch 00030: val_loss did not improve from 0.12400\n",
      "147/147 [==============================] - 0s 595us/sample - loss: 0.1213 - val_loss: 0.1246\n",
      "Epoch 31/200\n",
      "128/147 [=========================>....] - ETA: 0s - loss: 0.1214\n",
      "Epoch 00031: val_loss did not improve from 0.12400\n",
      "147/147 [==============================] - 0s 594us/sample - loss: 0.1207 - val_loss: 0.1241\n",
      "Epoch 32/200\n",
      "128/147 [=========================>....] - ETA: 0s - loss: 0.1209\n",
      "Epoch 00032: val_loss did not improve from 0.12400\n",
      "147/147 [==============================] - 0s 582us/sample - loss: 0.1204 - val_loss: 0.1242\n",
      "Epoch 33/200\n",
      "128/147 [=========================>....] - ETA: 0s - loss: 0.1210\n",
      "Epoch 00033: val_loss improved from 0.12400 to 0.12379, saving model to tmp_checkpoint.h5\n",
      "147/147 [==============================] - 0s 764us/sample - loss: 0.1198 - val_loss: 0.1238\n",
      "Epoch 34/200\n",
      "128/147 [=========================>....] - ETA: 0s - loss: 0.1186\n",
      "Epoch 00034: val_loss improved from 0.12379 to 0.12365, saving model to tmp_checkpoint.h5\n",
      "147/147 [==============================] - 0s 908us/sample - loss: 0.1190 - val_loss: 0.1236\n",
      "Epoch 35/200\n",
      " 80/147 [===============>..............] - ETA: 0s - loss: 0.1162\n",
      "Epoch 00035: val_loss improved from 0.12365 to 0.12360, saving model to tmp_checkpoint.h5\n",
      "147/147 [==============================] - 0s 1ms/sample - loss: 0.1188 - val_loss: 0.1236\n",
      "Epoch 36/200\n",
      " 80/147 [===============>..............] - ETA: 0s - loss: 0.1203\n",
      "Epoch 00036: val_loss improved from 0.12360 to 0.12303, saving model to tmp_checkpoint.h5\n",
      "147/147 [==============================] - 0s 1ms/sample - loss: 0.1183 - val_loss: 0.1230\n",
      "Epoch 37/200\n",
      " 80/147 [===============>..............] - ETA: 0s - loss: 0.1157\n",
      "Epoch 00037: val_loss improved from 0.12303 to 0.12301, saving model to tmp_checkpoint.h5\n",
      "147/147 [==============================] - 0s 1ms/sample - loss: 0.1177 - val_loss: 0.1230\n",
      "Epoch 38/200\n",
      "144/147 [============================>.] - ETA: 0s - loss: 0.1179\n",
      "Epoch 00038: val_loss did not improve from 0.12301\n",
      "147/147 [==============================] - 0s 933us/sample - loss: 0.1173 - val_loss: 0.1233\n",
      "Epoch 39/200\n",
      "128/147 [=========================>....] - ETA: 0s - loss: 0.1174\n",
      "Epoch 00039: val_loss did not improve from 0.12301\n",
      "147/147 [==============================] - 0s 575us/sample - loss: 0.1173 - val_loss: 0.1233\n",
      "Epoch 40/200\n",
      "128/147 [=========================>....] - ETA: 0s - loss: 0.1154\n",
      "Epoch 00040: val_loss did not improve from 0.12301\n",
      "147/147 [==============================] - 0s 614us/sample - loss: 0.1163 - val_loss: 0.1237\n",
      "Epoch 41/200\n",
      "128/147 [=========================>....] - ETA: 0s - loss: 0.1153\n",
      "Epoch 00041: val_loss did not improve from 0.12301\n",
      "147/147 [==============================] - 0s 565us/sample - loss: 0.1152 - val_loss: 0.1248\n",
      "Epoch 42/200\n",
      "128/147 [=========================>....] - ETA: 0s - loss: 0.1146\n",
      "Epoch 00042: val_loss did not improve from 0.12301\n",
      "147/147 [==============================] - 0s 582us/sample - loss: 0.1151 - val_loss: 0.1256\n"
     ]
    }
   ],
   "source": [
    "loss = Huber()\n",
    "optimizer = Adam(0.0005)\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "checkpoint = ModelCheckpoint(weight_name, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "history = model.fit(x_train, y_train, \n",
    "                    epochs=200, \n",
    "                    batch_size=16,\n",
    "                    validation_data=(x_valid, y_valid), \n",
    "                    callbacks=[early_stop, checkpoint])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1511f024a4443f7e5f12ae3dcc5576141864db34ee7c246ddb8e44c131c19028"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('prediction')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
